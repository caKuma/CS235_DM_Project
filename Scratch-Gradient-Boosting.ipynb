{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6909784,"sourceType":"datasetVersion","datasetId":3968343}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing all the libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport json\nimport re\nimport optuna\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nimport math\nimport scipy.stats as stats\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2023-12-16T01:36:19.512042Z","iopub.execute_input":"2023-12-16T01:36:19.512848Z","iopub.status.idle":"2023-12-16T01:36:20.155203Z","shell.execute_reply.started":"2023-12-16T01:36:19.512796Z","shell.execute_reply":"2023-12-16T01:36:20.154363Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Defining all the utility functions","metadata":{}},{"cell_type":"code","source":"\n# standardize the dataset \ndef standard_scale(X):\n    \"\"\"\n    Scale the features in the dataset X to have mean 0 and std 1.\n    Returns the scaled dataset.\n    \"\"\"\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)\n    return (X - means) / stds\n\n\ndef mean_absolute_error(y_true, y_pred):\n    absolute_errors = [abs(true - pred) for true, pred in zip(y_true, y_pred)]\n    mae = sum(absolute_errors) / len(absolute_errors)\n    return mae","metadata":{"execution":{"iopub.status.busy":"2023-12-16T01:36:21.006085Z","iopub.execute_input":"2023-12-16T01:36:21.007010Z","iopub.status.idle":"2023-12-16T01:36:21.013819Z","shell.execute_reply.started":"2023-12-16T01:36:21.006970Z","shell.execute_reply":"2023-12-16T01:36:21.012590Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Here we defining the XGBoost Model here","metadata":{}},{"cell_type":"code","source":"\n#xgboost from scratch\nclass XGBoostModel():\n    def __init__(self, params, random_seed=None):\n        self.params = defaultdict(lambda: None, params)\n        self.subsample = self.params['subsample'] \n        self.learning_rate = self.params['learning_rate'] \n        self.base_prediction = self.params['base_score'] \n        self.max_depth = self.params['max_depth'] \n        self.rng = np.random.default_rng(seed=random_seed)\n                \n    def fit(self, X, y, objective, iteration):\n        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n        self.boosters = []\n        for i in range(iteration):\n            gradients = objective.gradient(y, current_predictions)\n            hessians = objective.hessian(y, current_predictions)\n            sample_idxs = None if self.subsample == 1.0 \\\n                else self.rng.choice(len(y), \n                                     size=math.floor(self.subsample*len(y)), \n                                     replace=False)\n            booster = miniTrees(X, gradients, hessians, \n                                  self.params, self.max_depth, sample_idxs)\n            current_predictions += self.learning_rate * booster.predict(X)\n            self.boosters.append(booster)\n            \n    def predict(self, X):\n        ensemble_score=np.sum([booster.predict(X) for booster in self.boosters], axis=0)\n        return (self.base_prediction + self.learning_rate * ensemble_score)\n    \nclass miniTrees():\n \n    def __init__(self, X, g, h, params, max_depth, idxs=None):\n        self.params = params\n        self.l1_reg = params.get('lambda', 1)  # L1 regularization\n        self.l2_reg = params.get('alpha', 1)   # L2 regularization\n        self.max_depth = max_depth\n        self.min_child_weight = params['min_child_weight'] \n        self.gamma = params['gamma']\n        self.colsample_bynode = params['colsample_bynode'] \n        \n        if idxs is None: \n            idxs = np.arange(len(g))\n        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = -g[idxs].sum() / (h[idxs].sum() + self.l1_reg) \n        self.temp_best_score = 0.\n        if self.max_depth > 0:\n            self.insert_child_nodes()\n\n    def insert_child_nodes(self):\n        for i in range(self.c): self.find_split(i)\n        if self.is_leaf: return\n        x = self.X[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x <= self.threshold)[0]\n        right_idx = np.nonzero(x > self.threshold)[0]\n        self.left = miniTrees(self.X, self.g, self.h, self.params, \n                                self.max_depth - 1, self.idxs[left_idx])\n        self.right = miniTrees(self.X, self.g, self.h, self.params, \n                                 self.max_depth - 1, self.idxs[right_idx])\n\n    @property\n    def is_leaf(self): \n        return self.temp_best_score == 0.\n    \n    def find_split(self, feature_idx):\n        x = self.X[self.idxs, feature_idx]\n        g, h = self.g[self.idxs], self.h[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n        sum_g, sum_h = g.sum(), h.sum()\n        sum_g_right, sum_h_right = sum_g, sum_h\n        sum_g_left, sum_h_left = 0., 0.\n\n        for i in range(0, self.n - 1):\n            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n            sum_g_left += g_i; sum_g_right -= g_i\n            sum_h_left += h_i; sum_h_right -= h_i\n            if sum_h_left < self.min_child_weight or x_i == x_i_next:continue\n            if sum_h_right < self.min_child_weight: break\n                \n            \n            gain_left = (sum_g_left**2 / (sum_h_left + self.l2_reg)) - self.l1_reg * abs(sum_g_left)\n            gain_right = (sum_g_right**2 / (sum_h_right + self.l2_reg)) - self.l1_reg * abs(sum_g_right)\n            gain_total = (sum_g**2 / (sum_h + self.l2_reg)) - self.l1_reg * abs(sum_g)\n            \n            #this is the gain formula given in research paper\n            gain = 0.5 * (gain_left + gain_right - gain_total) - self.gamma / 2 \n            if gain > self.temp_best_score: \n                self.split_feature_idx = feature_idx\n                self.temp_best_score = gain\n                self.threshold = (x_i + x_i_next) / 2\n                \n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in enumerate(X)])\n    \n    # recursively predicting rows\n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n        return child._predict_row(row)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T01:43:44.620954Z","iopub.execute_input":"2023-12-16T01:43:44.621334Z","iopub.status.idle":"2023-12-16T01:43:44.650017Z","shell.execute_reply.started":"2023-12-16T01:43:44.621308Z","shell.execute_reply":"2023-12-16T01:43:44.648986Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"# Load the saved fold indices from the JSON file\nwith open('/kaggle/input/dm-dataset-2/k_fold_10.json', 'r') as file:\n    fold_indices = json.load(file)\n    \ndf=pd.read_csv('/kaggle/input/dm-dataset-2/model_dataset.csv')\nscore=df['score']\nnew_names = {col: re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns}\nnew_n_list = list(new_names.values())\nnew_names = {col: f'{new_col}_{i}' if new_col in new_n_list[:i] else new_col for i, (col, new_col) in enumerate(new_names.items())}\ndf = df.rename(columns=new_names)\n\nclass XGBloss():\n    def loss(self, y, pred): return np.mean((y - pred)**2)\n    def gradient(self, y, pred): return pred - y\n    def hessian(self, y, pred): return np.ones(len(y))\n\n    \n# 'fold_indices' now contains the loaded fold indices\nY=score\nX=df.drop(columns=['id','score'])\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, Y, test_size = 0.1, random_state=40)\n\nmae=[]\n\n# using off the shelf XGB boost here too\n\nimport xgboost as xgb\nparams_xgb = {'reg_alpha': 0.005267890553504806, 'reg_lambda': 0.274995940556105, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.01, 'max_depth': 10, 'eta': 0.01176581662520899, 'gamma': 5.460322647793291}\nparams_xgb['random_state'] = 42\nparams_xgb['n_estimators'] = 5000\n\n\nfor fold_number, fold_data in enumerate(fold_indices):\n    train_index = fold_data['train_indices']\n    test_index = fold_data['test_indices']\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = Y[train_index], Y[test_index]\n    params = {\n    'learning_rate': 0.01,\n    'max_depth': 10,\n    'subsample': 0.6,\n    'gamma': 5.46,\n    'min_child_weight': 25,\n    'base_score': 0.0,\n    'tree_method': 'exact',\n     'alpha': 0.005267890553504806,\n     'lambda' : 0.274995940556105,\n    'colsample_bynode': 1.0\n    }\n    num_boost_round = 1000\n\n    # train the from-scratch XGBoost model\n    model_scratch = XGBoostModel(params, random_seed=42)\n    model_scratch.fit(X_train, y_train, XGBloss(), num_boost_round)\n    pred_scratch = model_scratch.predict(X_test)\n    \n    # off the shelf \n    model_x = xgb.XGBRegressor(predictor='gpu_predictor',\n        n_jobs=4,**params_xgb )\n    model_x.fit(X_train,y_train, verbose = False )\n\n    mae.append(mean_absolute_error(y_test, pred_scratch))\n    print(f'Fold {fold_number+1} --- ')\n    \n    \n\n    print (\"Mean absolute Error:\", mae[-1])\n    \n    \n    if fold_number==9:\n        final_pred_off=model_x.predict(X_test_full)\n        final_pred_scratch=model_scratch.predict(X_test_full)\n        tstat, t_pval = stats.ttest_ind(a=final_pred_scratch, b=final_pred_off, equal_var=True)\n        print (f\" Final -->  T-Statistics  {tstat} -- P-Value {t_pval}\")\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-16T01:48:12.016424Z","iopub.execute_input":"2023-12-16T01:48:12.017375Z","iopub.status.idle":"2023-12-16T02:51:49.091304Z","shell.execute_reply.started":"2023-12-16T01:48:12.017337Z","shell.execute_reply":"2023-12-16T02:51:49.090360Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Fold 1 --- \nMean absolute Error: 0.5574640871400883\nFold 2 --- \nMean absolute Error: 0.5052909959161523\nFold 3 --- \nMean absolute Error: 0.5056886210888177\nFold 4 --- \nMean absolute Error: 0.5435406704103566\nFold 5 --- \nMean absolute Error: 0.4916269194616932\nFold 6 --- \nMean absolute Error: 0.48622284634535906\nFold 7 --- \nMean absolute Error: 0.5001712185648743\nFold 8 --- \nMean absolute Error: 0.497330921375785\nFold 9 --- \nMean absolute Error: 0.4837388482477115\nFold 10 --- \nMean absolute Error: 0.488158193327983\n Final -->  T-Statistics  0.040314194890341326 -- P-Value 0.9678589907375328\n","output_type":"stream"}]},{"cell_type":"code","source":"np.mean(mae)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T02:52:10.252770Z","iopub.execute_input":"2023-12-16T02:52:10.253430Z","iopub.status.idle":"2023-12-16T02:52:10.260707Z","shell.execute_reply.started":"2023-12-16T02:52:10.253399Z","shell.execute_reply":"2023-12-16T02:52:10.259623Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0.5059233321878821"},"metadata":{}}]},{"cell_type":"markdown","source":"# The final Mean Absolute Value is 0.5059","metadata":{}},{"cell_type":"markdown","source":"# Here we defining LGBM Model ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nclass LGBMModel:\n    def __init__(self, num_iterations=100, learning_rate=0.07, max_depth=5):\n        self.num_iterations = num_iterations\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n\n    def fit(self, X, y):\n        predictions = np.zeros(len(y))\n        for _ in range(self.num_iterations):\n            residuals = y - predictions  # Calculate residuals\n            gradients = -residuals  # Negative residuals for MSE\n            hessians = np.ones(len(y))  # Constant hessian for MSE\n\n            # Ensure gradients is a NumPy array\n            gradients = np.array(gradients)\n            residuals = np.array(residuals)\n\n            # Sorting the data based on absolute gradients\n            sorted_indices = np.argsort(np.abs(gradients))[::-1]\n\n            # Selecting the top portion with large gradients\n            top_fraction = int(0.2 * len(gradients))\n            top_set = sorted_indices[:top_fraction]\n\n            # Randomly sampling from the remaining data\n            random_fraction = int(0.5 * (len(gradients) - top_fraction))\n            random_set = np.random.choice(sorted_indices[top_fraction:], size=random_fraction, replace=False)\n\n            selected_indices = np.concatenate([top_set, random_set])\n\n            X_selected, residuals_selected, gradients_selected, hessians_selected = X[selected_indices], residuals[selected_indices], gradients[selected_indices], hessians[selected_indices]\n\n            # Scaling factor for smaller gradients\n            scaling_factor = len(gradients) / max(1, len(random_set))\n            for i in range(len(selected_indices)):\n                if selected_indices[i] in random_set:\n                    gradients_selected[i] *= scaling_factor\n\n            tree = LightTrees(X_selected, residuals_selected, gradients_selected, hessians_selected, self.max_depth)\n            tree.build_tree()\n            self.trees.append(tree)\n\n            # Updating predictions\n            predictions += self.learning_rate * tree.predict(X)\n\n\n    def predict(self, X):\n        predictions = np.zeros(X.shape[0])\n        for tree in self.trees:\n            predictions += self.learning_rate * tree.predict(X)\n        return predictions\n\n    \nclass LightTrees:\n    def __init__(self, X,residuals, gradients,hessians, max_depth=3):\n        self.X = X\n        self.gradients = gradients\n        self.hessians = hessians\n        self.max_depth = max_depth\n        self.residuals=residuals\n        self.tree = {}\n        self.next_unused_node_id = 0\n        self.feature_importances_ = np.zeros(X.shape[1])\n\n    def build_tree(self):\n        self._split(0, self.X, self.residuals,self.hessians,depth=1)\n        \n    def _split(self, node_id, X, gradients, hessians, depth):\n        if depth > self.max_depth or len(np.unique(gradients)) <= 1:\n            self.tree[node_id] = {'value': np.mean(gradients)}\n            return\n\n        best_feature, best_threshold = self.find_best_split(X, gradients, hessians)\n        if best_feature is None:\n            self.tree[node_id] = {'value': np.mean(gradients)}\n            return\n\n        # Create masks for left and right splits\n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = X[:, best_feature] > best_threshold\n\n        # Checking if the split is valid\n        if not np.any(left_mask) or not np.any(right_mask):\n            self.tree[node_id] = {'value': np.mean(gradients)}\n            return\n        \n        left_node_id = self.next_unused_node_id + 1\n        right_node_id = self.next_unused_node_id + 2\n        self.next_unused_node_id += 2\n\n        # Add the current node's split info to the tree\n        self.tree[node_id] = {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': left_node_id,\n            'right': right_node_id\n        }\n\n        # splitting the tree using recursion\n        self._split(left_node_id, X[left_mask], gradients[left_mask], hessians[left_mask], depth + 1)\n        self._split(right_node_id, X[right_mask], gradients[right_mask], hessians[right_mask], depth + 1)\n\n\n\n    def find_best_split(self, X, gradients, hessians):\n        num_bins = 10\n        best_feature = None\n        best_threshold = None\n        best_gain = float('-inf')\n\n        # Sample a subset of features\n        num_features = X.shape[1]\n        subset_features = np.random.choice(num_features, size=int(0.8 * num_features), replace=False)\n\n        for feature_idx in subset_features:\n            bin_edges = np.histogram_bin_edges(X[:, feature_idx], bins=num_bins)\n            \n            for i in range(len(bin_edges) - 1):\n                threshold = (bin_edges[i] + bin_edges[i + 1]) / 2\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = X[:, feature_idx] > threshold\n\n                if not np.any(left_mask) or not np.any(right_mask):\n                    continue\n\n                gain = self.calculate_gain(gradients[left_mask], hessians[left_mask],\n                                           gradients[right_mask], hessians[right_mask])\n\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n\n        return best_feature, best_threshold\n\n    def calculate_gain(self, left_gradients, left_hessians, right_gradients, right_hessians, lambda_=0.3):\n        sum_left_gradients = np.sum(left_gradients)\n        sum_left_hessians = np.sum(left_hessians) + lambda_\n        sum_right_gradients = np.sum(right_gradients)\n        sum_right_hessians = np.sum(right_hessians) + lambda_\n\n        # Gain calculation\n        gain = 0.5 * (\n            sum_left_gradients ** 2 / sum_left_hessians + \n            sum_right_gradients ** 2 / sum_right_hessians - \n            (sum_left_gradients + sum_right_gradients) ** 2 / (sum_left_hessians + sum_right_hessians)\n        )\n        return gain\n\n    def predict(self, X):\n        predictions = np.array([self._predict_row(x, 0) for x in X])\n        return predictions\n\n    def _predict_row(self, x, node_id):\n        node = self.tree.get(node_id, {})\n        if 'value' in node:\n            return node['value']\n        if x[node['feature']] <= node['threshold']:\n            return self._predict_row(x, node['left'])\n        else:\n            return self._predict_row(x, node['right'])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T06:22:35.458457Z","iopub.execute_input":"2023-12-16T06:22:35.458838Z","iopub.status.idle":"2023-12-16T06:22:35.489823Z","shell.execute_reply.started":"2023-12-16T06:22:35.458808Z","shell.execute_reply":"2023-12-16T06:22:35.488954Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"\n# Load the saved fold indices from the JSON file\nwith open('/kaggle/input/dm-dataset-2/k_fold_10.json', 'r') as file:\n    fold_indices = json.load(file)\n    \ndf=pd.read_csv('/kaggle/input/dm-dataset-2/model_dataset.csv')\nscore=df['score']\nnew_names = {col: re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns}\nnew_n_list = list(new_names.values())\nnew_names = {col: f'{new_col}_{i}' if new_col in new_n_list[:i] else new_col for i, (col, new_col) in enumerate(new_names.items())}\ndf = df.rename(columns=new_names)\n\nclass LightObjective():\n    def loss(self, y, pred): return np.mean((y - pred)**2)\n    def gradient(self, y, pred): return pred - y\n    def hessian(self, y, pred): return np.ones(len(y))\n\n    \n# 'fold_indices' now contains the loaded fold indices\nY=score\nX=df.drop(columns=['id','score'])\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# LGBM off the shelf \nimport lightgbm as lgb\nparams_l = {'reg_alpha': 0.00696289257129596,\n 'reg_lambda': 0.3111656149412144,\n 'colsample_bytree': 0.7,\n 'subsample': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 25,\n 'num_leaves': 16,\n 'min_child_samples': 11}\nparams_l['random_state'] = 42\nparams_l['n_estimators'] = 5000\nparams_l['metric'] = 'mae'\nparams_l['objective']='regression'\n\nmae=[]\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, Y, test_size = 0.1, random_state=40)\n\nfor fold_number, fold_data in enumerate(fold_indices):\n    train_index = fold_data['train_indices']\n    test_index = fold_data['test_indices']\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = Y[train_index], Y[test_index]\n\n    # train the from-scratch XGBoost model\n    model_scratch = LGBMModel()\n    model_scratch.fit(X_train, y_train)\n    pred_scratch = model_scratch.predict(X_test)\n\n    mae.append(mean_absolute_error(y_test, pred_scratch))\n    \n    print(f'Fold {fold_number+1} --- ')\n    \n    model_l = lgb.LGBMRegressor(objective='regression',metric='mae',  n_estimators =5000)\n    model_l.fit(X_train,y_train)\n    \n\n    print (\"Mean absolute Error:\", mae[-1])\n    \n    \n    if fold_number==9:\n        final_pred_off=model_l.predict(X_test_full)\n        final_pred_scratch=model_scratch.predict(X_test_full)\n        tstat, t_pval = stats.ttest_ind(a=final_pred_scratch, b=final_pred_off, equal_var=True)\n        print (f\" Final -->  T-Statistics - {tstat} -- P-Value {t_pval}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-16T06:22:36.033178Z","iopub.execute_input":"2023-12-16T06:22:36.033792Z","iopub.status.idle":"2023-12-16T06:35:10.730983Z","shell.execute_reply.started":"2023-12-16T06:22:36.033756Z","shell.execute_reply":"2023-12-16T06:35:10.729987Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Fold 1 --- \nMean absolute Error: 0.5684421645229054\nFold 2 --- \nMean absolute Error: 0.5290325367213622\nFold 3 --- \nMean absolute Error: 0.5234859170993434\nFold 4 --- \nMean absolute Error: 0.5523018319994341\nFold 5 --- \nMean absolute Error: 0.49387507623807997\nFold 6 --- \nMean absolute Error: 0.4731296636790384\nFold 7 --- \nMean absolute Error: 0.5200839752561888\nFold 8 --- \nMean absolute Error: 0.5028907699638577\nFold 9 --- \nMean absolute Error: 0.4757437734816401\nFold 10 --- \nMean absolute Error: 0.49963031306930833\n Final -->  T-Statistics - -0.027854116330203224 -- P-Value 0.977789798463584\n","output_type":"stream"}]},{"cell_type":"code","source":"np.mean(mae)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T06:36:24.995794Z","iopub.execute_input":"2023-12-16T06:36:24.996135Z","iopub.status.idle":"2023-12-16T06:36:25.002086Z","shell.execute_reply.started":"2023-12-16T06:36:24.996109Z","shell.execute_reply":"2023-12-16T06:36:25.001079Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"0.50547"},"metadata":{}}]},{"cell_type":"markdown","source":"# The Final MAE of LGBM is 0.50547","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}